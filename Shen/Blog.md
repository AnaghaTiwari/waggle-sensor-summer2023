# Monday June 12th, 2023
I did all of the trainings and student orientation, and I did not have time to start working on the project yet. 
I spent most of the day doing the training exams and went to have lunch at around 12. I was able to get a rundown from Seongha on what 
I should be doing for my project.

# Tuesday June 13th, 2023
I got started on preprocessing the data. I actually made more progress today than I thought. I was able to create my personal directory in github, and start the blog along with create another directory storing my progress in the project. I was able to convert the Solar Irradiance data that was given by Seongha into a CSV file(at first I attempted to use JSON but quickly found out that the formatting wrong and would take too long to format it correctly). I used the Pandas package in order to scrape the data from the CSV file (I also had to reformat the CSV File to only contain days from the summer along with formatting it correctly to show the correct amount of columns in Colabs). Furthermore, I also created a plot that shows the solar irradiance throughout the whole summer, so that I can get a better idea of how many classes I should use for my overall model.

# Wednesday June 14th, 2023
Today, I was notified that we should only be measuring solar irradiance during the certain times of the day when the sun is likely to be out rather than times at night. Furthermore, I cleaned all the data for the images in order to retain only the images that are timestamped between 6:00AM and 8:00PM using Pandas and the Date Time library from python. I coded a function that would only pick one image per 15 minutes so I can eventually match the images with each row of data from the CSV file. There is is still a disparity between the number of rows of data in the Solar Irradiance and the number of images that I processed; I will continue to work toward closing this disparity so the index of a row can correspond to both the image and the respective Solar Irradiance  


# Thursday June 15th, 2023
Today I went to my first team meeting, and it was very cool to hear about the projects that other interns were working on. I was able to introduce myself to the rest of them, and give them all an update on the progress I had made in my project. After the meeting, I worked to finish my student quadchart introduction slide, and also uploaded it onto my github folder. Throughout the day I was thinking about how to identify which images did not match up with the data chronologically since I want both dataframes to have the same timestamp in each row so that they can be matched up. I finished the algorithm to do so, but I received a bug that set me back about an hour and a half, which was an indexing error; because dataframes do not automatically reset the indexing after dropping rows from a dataframe, I had forgotten to reset the indices. However, by the end of the day I had figured it out, and now the matching of the data should be finished quickly.


# Friday June 16th, 2023
Today I worked my first remote day. I began the day by utilizing the function that I had thought of yesterday in order to finish cleaning the data up. I managed to be able to finally match the images with the solar irradiance data. I combined the dataframes into a final, cleaned up, dataframe where one column stores the paths to the images while the other column stores the respective radW/m^2 measurement. I also began to think of how I was going to store the images into classes/folders. I finalized the approach to storing these images on google drive by iterating through the dataframe and storing the images in respective folders with each range being 100, meaning each class would be 0-100, 100-200, etc. all the way till 1000-1100. I initially planned on using my Macbook to store the folders but quickly came to the realization that since I am working on colabs it had no access to my Mac's storage. I then pivoted by doing a bit of research and coming to the conclusion of using google drive, so I first connected the colab to my google drive by importing the package 'drive'. I then created a new class column in the dataframe to store whichever class each image belonged to. For next week, I plan on using this column 'class' to both create a folder and store the images respective to their irradiance value, 


# Monday June 19th, 2023

Today I worked on downloading all the images over to my local directory in their correct respective solar irradiance classes. I eventually figured it out but found that some images were still dark even though the time that they were taken were already filtered so that it removed darker images. This is when I was notified by Seongha that the images were taken in UTC time while the data were taken in CST time; granted, there was a feature on the sage website to check at which times the photos were taken, and I had completely forgotten to check if there was a timezone difference. So, I worked more on cleaning data and pairing up the images and the data based on their timestamps. I should be able to finish tomorrow since I have most of the functions to do it and downloading images should also be able to be done by tomorrow. 


# Tuesday June 20th, 2023

Today I finished cleaning up the final data that accounts for the timezone difference between the images and the data. I again used my function that I had created earlier in order to spot differences between the timestamps so that I could get rid of unnecessary either data or image that did not have a corresponding image/data. I also managed to download all of these on to my local machine by writing a python script that sorts each image into a directory based on a the column 'class' in the dataframe. Seongha has notified me that I should use the resnet50 pretrained model in pytorch for my first model. After I downloaded these images onto my local machine and uploaded them onto google drive, I began researching how I plan to finetune the model in order to get my first test runs going.  

# Wednesday June 21st, 2023

Today I attended two of the required seminars for SULI students. Then I began to do more research on how to exactly train the last layer of the model. After reading a few tutorials I got started and managed to finish a rough first model using pytorch and resnet18 model. This first model did not go exactly as planned since the accuracy was very very low. I think that I just do not have enough data for each class so it makes the accuracy very low especially since for about half the classes I only have about 200-300 images. I plan on trying to either reduce the number of classes or augment the images to have more data per class.


# Thursday June 22nd, 2023
Today I attended the team meeting and was able to present the team with my progress. I was suggested the possibility that the image may not be as good of an indicator as we would have thought for predicting the solar irradiance. I believe that we may need to reduce the # of classes in order to get a model that can somewhat accurately make predictions. Today, I quadrupled my initial 10 class dataset by rotating the images 90, 180, and 270 degrees. I will first try training the model on a larger set of data before I reduce my classes, but I believe that the image discrepancy between a class like 0-100 w/m2 compared to 100-200 w/m2 may be too small to the point where the machine cannot learn anything. Thus I may consider switching to a larger range of classes so that the images may actually have a visual difference. I also updated the validation splits after I increased my data set so that it maintained the 80/20 split. Uploading to google drive took a bit longer than expected, so I think that I will test my larger data set tomorrow.


# Friday June 23rd, 2023
Today I tested my new larger data set and I saw about the same results. However, the model with 5 classes did a bit better with about a 4% increase in validation accuracy to 70%. I now believe that it is not the size of the data set that is the issue but rather the data itself. I think the images are not clear enough as to how the model is learning through the images. It seems that it is not able to tell a clear difference based on the raw image alone. I utilized the cv2 library to augment the image by increasing the contrast in the image. I also utilized this library to make the sun a bit brighter in each image. I think that I will use a model that processes 5 classes for now instead and test it out with 10 classes at a later point should this attempt succeed.

# Monday June 26th, 2023
Today my goal was to successfully utilize my functions to augment images to augment them in place within the folders and to hopefully download them and process them into colabs by the end of the day. I first worked on using the function to augment the contrast and sun's lighting within the folders and I did this using python libraries like cv2 and os. First I rotated every image by 60 degrees so that I 6x'ed my data set and then I augmented the contrast within each image to hopefully assist the model in generalizing gradually through the cloud cover. Most of the time today was spent waiting for the images to download so while this was happening I did more research on tuning hyper parameters so that my model would be able to gain even a marginally higher validation accuracy.

# Tuesday June 27th, 2023
Today I successfully finished downloading the augmented images (rotated and enhanced contrast) into my google drive to run in my model. I also tuned some hyperparameters in my model like the batchsize to be 16 instead of 32. Unfortunately the model took way too long to run, so I ran out of GPU runtime in colabs. I figured this was strange since it only ran for 1 epoch. It did achieve a 60% validation accuracy on just the first epoch so it does look more promising than before. I did some research on why my colabs was taking so long to run and I found out that it was because I was trying to load the images directly whereas unzipping the files directly into the colabs VM would supposedly take much less time. I have now zipped a folder and am looking to test the speed of the model training tomorow. Hopefully this will allow me to tune parameters much faster and to find the best optimization for my model 

# Wednesday June 28th, 2023
I now have all the zip files in my google drive and unzip them in colabs. This allows me to run the larger datasets much quicker. Each epoch is completed about every 15 minutes. Today I experimented with the 10 class dataset and found that the model now learns something. I believe it is because of the larger dataset and because I removed the random crop feature from the initial preprocessing. I think that the random crop of the image needed to be taken out so that the model can correctly identify differences from each other image since lots of the images look similar, it needs to be able to distinguish the various images. The Resnet model had a training accuracy of about 98% but validation was only 40%; while not optimal, it is definitely progress since it seems the model can learn even if it is only the training accuracy. I tested this with Resnet34, and it performed at about 60% validation accuracy. A problem that I keep having is that Colabs will randomly disconnect and interrupt my training completely. I think the next steps are to fix the disconnecting issue and to fine tune the model more to gain a higher validation accuracy.

# Friday June 29th, 2023
Today I trained two models. I first tried to add a random rotation between -50 to 50 degrees to all the images on a resnet18 pretrained model but this unfortunately only resulted in the same 60% validation accuracy, I do not know why this is, but maybe it is something to do with the position of the sun? Though my data in the training data set includes rotated images so I am not very sure. I may try to train the model without the rotated images in my future trials. I also trained the same resnet18 model on images that were randomly horizontally flip but no rotation and this resulted in a validation accuracy of about 67% which was a bit better. I think I may try to take out the extra rotated images that I had previously added. I would like to find out if the position of the sun is perhaps taken into account by the model 


# Wednesday July 5th, 2023
Today I tested cutting the images into circles, making the background transparent, but this forced me to store the images as pngs which are lossless and thus are too big of files to even zip up to my google drive and was taking up far too much space on my icloud. I also tested the model with 5 classes without the cropped images and it resulted in a decent initial validation accuracy of about 67% but did not increase for some reason. I also simultaneously worked on testing the 10 class model again. I am now trying to balance the classes so that each class would be able to train on a similar amount of images; I am hoping this will help with overfitting. I also attended two meetings today, the mandatory UCP weekly seminar and a research meeting with Daniele. These meetings were both very interesting to listen to.  

# Friday July 7th, 2023
Today my goal was to try to find a way to speed up my training process since each epoch was currently taking 15 minutes so 20 epoch trainings were taking upward of 4 hours. Sean and Seongha helped me out with this in the previous day when they recommended to shrink each 2000x2000 pixel image to 500x500. This ended up working and now each epoch is taking only about 3 minutes which is much faster. I utilized a python script to help me shrink all the images. I also tested out balancing the classes today and it resulted in decent results since I decreased the size of my overall data set by balancing out the # of images per class. This resulted in a slight increase in accuracy. My next step is simply to try and increase my current data set size through augmentation since my trainings now take a much shorter time. I plan to utilize rotations and enhanced contrast in order to help my model generalize better. 

# Monday July 10th, 2023
Today my goal was just to experiment with different augmentations to make my data set bigger and seeing whether this would help since Sean and Seongha helped me heavily in discovering how to make my trainings almost 4x faster. I first ran the regular 10 class data set with 1000 images each where each class had a few rotated images with both a resnet34 and resnet50; the resnet 50 outperformed the resnet 34 by about 6% accuracy. However the resnet50 was still only 77% accuracy. I then experimented with doubling the size of my data set where the increase was going to be from increasing the contrast of the sky in the image in half of the original dataset and with the other half I would increase the sun's contrast against the background. I performed the same training on the model with the new dataset where the images are randomly horizontally flipped and rotated at a random angle in the preprocessing of the data. This brought about very promising results where accuracy reached about 90%. 

# Tuesday July 11th, 2023
Today I wanted to tune the model closer to optimization so that I could start on my plugin. I first began by trying to retrain my model to confirm the convergence at 90% and found that it actually converged at 92%, but I needed to change the validation images back to 2048x2048 pixels since the model will be trying to predict images of those size. So I changed the images back to 2048 pixels but I do not know if the quality of the images were restored. I believe some information may have been lost and this may harm the accuracy but I am now testing resnet18, resnet34 and resnet 50 models on this dataset. 

# Wednesday July 12th, 2023
Today I wanted to tune a few parameters to find optimization of the model. I ran the model with different parameters about 4-5 times today. On one of the models I made the validation image size slightly larger but this did not make a difference which in hindsight makes sense since the model is not training on that data. I also ran the model without rotating the images in preprocessing but this resulted in much lower accuracy so I plan to run the model with a random rotation within 360 degrees instead of 300. I also ran the model on inputted validation images that were original size which resulted in 90% accuracy about 2% lower than with the 500x500 pixel validation images. I believe it is because the images are a different size than the training which results in lower validation accuracy but ultimately it is not a huge difference and shows that the model is able to predict 2048x2048 pixel images even though the model is trained on 500x500 images. 

# Friday July 14th, 2023
Yesterday I was notified after I had started working on my testing split data to try a new approach with a regression model so that the output that my model spits back will be a continuous numerical value instead of the bins I had used earlier. As for today, I started by going back to the dataframe that I had made in week 1, downloaded all the images and resized them, then I had to think for a bit about how to properly create the csv file with the paths to the images, irradiance values, and validation booleans. I ended up doing this in colab. I zipped up all the resized images and created a dataframe in colab and wrote a few scripts to insert the file paths that are needed for the model into the data frame and used the filename of the images to label them with irradiance values. I then converted the file to csv and uploaded into my github so that I can load the csv into colab through github.

# Monday July 17th, 2023
Today, I got started on writing the framework for my new regression model using the csv file that I had written the previous week. I looked through many websites and tutorials and eventually found a framework that I could use as an example, but I had to make a few tweaks to calculate the correct validation loss. The validation loss was originally way too high and way too low, but after some experimentation I got the average to work.
